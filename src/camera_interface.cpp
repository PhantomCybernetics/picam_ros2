#include <chrono>
#include <future>
#include <memory>
#include <string>

#include <libcamera/libcamera.h>
#include <libcamera/pixel_format.h>

#include <sys/mman.h>
#include <unistd.h>

#include "picam_ros2/picam_ros2.hpp"
#include "picam_ros2/camera_interface.hpp"

#include "rclcpp/rclcpp.hpp"
#include "std_msgs/msg/header.hpp"
#include "builtin_interfaces/msg/time.hpp"
#include "ffmpeg_image_transport_msgs/msg/ffmpeg_packet.hpp"

extern "C" {
    #include <libavcodec/avcodec.h>
    #include <libavcodec/codec_id.h>
    #include <libavutil/opt.h>
    #include <libavutil/imgutils.h>
    #include <libswscale/swscale.h>
}

using namespace libcamera;

CameraInterface::CameraInterface(std::shared_ptr<Camera> camera, std::shared_ptr<PicamROS2> node) {
    std::cout << "Initiating " << camera->id() << std::endl; 
    this->camera = camera;
    this->node = node;

    this->publisher = this->node->create_publisher<ffmpeg_image_transport_msgs::msg::FFMPEGPacket>("/picam_test_c", 1);

    if (!this->initializeEncoder()) {
        std::cerr << "Error initializing encoder" << std::endl; 
        return;
    }

    if(avcodec_open2(this->codec_context, this->codec, nullptr) < 0){
        std::cerr << "Could not open codec" << std::endl;
        return;
    }

    this->camera->acquire();

    std::unique_ptr<CameraConfiguration> config = this->camera->generateConfiguration( { StreamRole::VideoRecording } );
    config->validate();
    
    this->streamConfig = config->at(0);
    std::cout << YELLOW << "Stream config: " << this->streamConfig.toString() << CLR << std::endl; 
    std::cout << YELLOW << "Stride: " << this->streamConfig.stride << CLR << std::endl; 
    this->camera->configure(config.get());

    FrameBufferAllocator *allocator = new FrameBufferAllocator(this->camera);

    std::cout << "Allocating" << std::endl;
    for (StreamConfiguration &cfg : *config) {
        auto stream = cfg.stream();
        int ret = allocator->allocate(stream);
        if (ret < 0) {
            std::cerr << "Can't allocate buffers" << std::endl;
            return;
        }
        const std::vector<std::unique_ptr<FrameBuffer>> &buffers = allocator->buffers(stream);
        size_t allocated = buffers.size();
        std::cout << "Allocated " << allocated << " buffers for stream pixel format: " << cfg.pixelFormat.toString() << std::endl;

        for (unsigned int i = 0; i < buffers.size(); ++i) {
            std::unique_ptr<Request> request = camera->createRequest();
            if (!request)
            {
                std::cerr << "Can't create request" << std::endl;
                return;
            }

            const std::unique_ptr<FrameBuffer> &buffer = buffers[i];
            int ret = request->addBuffer(stream, buffer.get());
            if (ret < 0)
            {
                std::cerr << "Can't set buffer for request" << std::endl;
                return;
            }
            this->requests.push_back(std::move(request));
        }
    }

    this->camera->requestCompleted.connect(this, &CameraInterface::requestComplete);
    camera->start();
    for (std::unique_ptr<Request> &request : this->requests)
        camera->queueRequest(request.get());
}

bool CameraInterface::initializeEncoder() {
    // Initialize encoder
    // this->codec = avcodec_find_encoder(AV_CODEC_ID_H264); //CPU
    this->codec = avcodec_find_encoder_by_name("h264_v4l2m2m"); // hw encoder on bcm2835
    if (!this->codec){
        std::cerr << "Codec with specified id not found" << std::endl;
        return false;
    }
    this->codec_context = avcodec_alloc_context3(codec);
    if (!this->codec_context){
        std::cerr << "Can't allocate video codec context" << std::endl;
        return false;
    }

    assert(this->width % 32 == 0 && "Width not aligned to 32");

    this->codec_context->profile = FF_PROFILE_H264_BASELINE;
    this->codec_context->height = this->height;
    this->codec_context->width = this->width;

    /// Frames per second
    this->codec_context->time_base.num = 1;
    this->codec_context->time_base.den = this->fps;
    this->codec_context->framerate.num = this->fps;
    this->codec_context->framerate.den = 1;

    this->codec_context->bit_rate = 4000000;

    /// Only YUV420P for H264|5
    this->codec_context->pix_fmt = AV_PIX_FMT_YUV420P;

    /// Key(intra) frame rate
    this->codec_context->gop_size = this->fps*2;

    /// P-frames, generated by referencing data from prev and future frames.
    /// [Compression up, CPU usage up]
    /// [use 3/gop]
    this->codec_context->max_b_frames = 0;

    /// Can be used by a P-frame(predictive, partial frame) to help define a future frame in a compressed video.
    /// [use 3â€“5 ref per P]
    this->codec_context->refs = 3;

    /// Compression efficiency (slower -> better quality + higher cpu%)
    /// [ultrafast, superfast, veryfast, faster, fast, medium, slow, slower, veryslow]
    /// Set this option to "ultrafast" is critical for realtime encoding
    av_opt_set(this->codec_context->priv_data, "preset", "ultrafast", 0);

    /// Compression rate (lower -> higher compression) compress to lower size, makes decoded image more noisy
    /// Range: [0; 51], sane range: [18; 26]. I used 35 as good compression/quality compromise. This option also critical for realtime encoding
    av_opt_set(this->codec_context->priv_data, "crf", "35", 0);

    /// Change settings based upon the specifics of input
    /// [psnr, ssim, grain, zerolatency, fastdecode, animation]
    /// This option is most critical for realtime encoding, because it removes delay between 1th input frame and 1th output packet.
    av_opt_set(this->codec_context->priv_data, "tune", "zerolatency", 0);

    auto desc = av_pix_fmt_desc_get(AV_PIX_FMT_YUV420P);
    if (!desc){
        std::cerr << "Can't get descriptor for pixel format for AV_PIX_FMT_YUV420P" << std::endl;
        return false;
    }
    this->bytes_per_pixel = av_get_bits_per_pixel(desc) / 8;
    
    std::cerr << CYAN << "Encoder initiated for " << this->width << "x" << this->height << " @ " << this->fps << " fps" << "; BPP=" << this->bytes_per_pixel << CLR << std::endl;

    return true;
}

builtin_interfaces::msg::Time get_current_stamp(uint64_t timestamp_ns) {
    // rclcpp::Time now = node->get_clock()->now();
    
    // Split into seconds and nanoseconds
    builtin_interfaces::msg::Time stamp;
    stamp.sec = static_cast<int32_t>(timestamp_ns / 1000000000);
    stamp.nanosec = static_cast<uint32_t>(timestamp_ns % 1000000000);;
    
    return stamp;
}

void CameraInterface::requestComplete(Request *request) {
    if (request->status() == Request::RequestCancelled)
        return;
    
    const std::map<const Stream *, FrameBuffer *> &buffers = request->buffers();

    for (auto bufferPair : buffers) {
        FrameBuffer *buffer = bufferPair.second;
        const FrameMetadata &metadata = buffer->metadata();

        // unsigned int nplane = 0;
        // std::cout << " seq: " << std::setw(6) << std::setfill('0') << metadata.sequence << " bytesused: ";
        // for (const FrameMetadata::Plane &plane : metadata.planes())  {
            // std::cout << plane.bytesused;
            // if (++nplane < metadata.planes().size()) std::cout << "/";
        // }
        // std::cout << std::endl;

        AVFrame *frame = av_frame_alloc();
        if (!frame){
            std::cerr << "Could not allocate video frame" << std::endl;
            return;
        }

        frame->format = this->codec_context->pix_fmt;
        frame->height = this->codec_context->height;
        frame->width = this->codec_context->width;

        const std::vector<libcamera::FrameBuffer::Plane>& planes = buffer->planes();

        size_t total_size = 0;
        for (size_t i = 0; i < planes.size(); ++i) {
            size_t plane_end = planes[i].offset + planes[i].length;
            if (plane_end > total_size) {
                total_size = plane_end;
            }
        }

        void* base = mmap(nullptr, total_size, PROT_READ, MAP_SHARED, planes[0].fd.get(), 0);
        if (base == MAP_FAILED) {
            std::cerr << "Failed to mmap entire buffer: " << strerror(errno) << std::endl;
            return;
        }

        auto free_buffer = [](void* opaque, uint8_t* data) {
            munmap(data, static_cast<size_t>(reinterpret_cast<uintptr_t>(opaque)));
        };

        std::cout << std::setw(6) << std::setfill('0') << metadata.sequence << ": ";
        for (size_t i = 0; i < planes.size(); ++i) {

            // Map the memory
            std::cout << "plane#" << i <<" ";
            if (i == 0)
                std::cout << "fd=" << planes[i].fd.get() << "; ";
            std::cout << planes[i].offset << "+" << planes[i].length << " ";

            frame->buf[i] = av_buffer_create(
                static_cast<uint8_t*>(base)+planes[i].offset,
                planes[i].length,
                free_buffer,
                reinterpret_cast<void*>(planes[i].length),
                0
            );

            // Assign plane data using offsets within the mapped buffer
            frame->data[i] = frame->buf[i]->data;
            frame->linesize[i] = (i == 0) ? this->streamConfig.stride : this->streamConfig.stride / 2;

            std::cout << "|" << frame->linesize[i] << "|";
            if (i < planes.size()-1)
                std::cout << "; ";
        }
        std::cout << std::endl;

        /// Set frame index in range: [1, fps]
        frame->pts = this->frameIdx;

        /// Set frame type
        bool isKeyFrame = false;
        if (isKeyFrame){
            frame->key_frame = 1;
            frame->pict_type = AVPictureType::AV_PICTURE_TYPE_I;
        }
        // std::cout << "Sending..." << std::endl;

        switch (avcodec_send_frame(this->codec_context, frame)){
            case 0:
                this->frameIdx = (this->frameIdx % this->codec_context->framerate.num) + 1;
                break;
            case AVERROR(EAGAIN):
                std::cerr << "Error sending frame to encoder: AVERROR(EAGAIN)" << std::endl;
                break;
            case AVERROR_EOF:
                std::cerr << "Error sending frame to encoder: AVERROR_EOF" << std::endl;
                return;
            case AVERROR(EINVAL):
                std::cerr << "Error sending frame to encoder: AVERROR(EINVAL)" << std::endl;
                return;
            case AVERROR(ENOMEM):
                std::cerr << "Error sending frame to encoder: AVERROR(ENOMEM)" << std::endl;
                return;
            default:
                std::cerr << "Error sending frame to encoder: Other error" << std::endl;
                return;
        }

        av_frame_free(&frame);

        AVPacket *packet = av_packet_alloc();
        if (packet == NULL) {
            std::cerr << "Error making packet" << std::endl;
            return;
        }

        switch (avcodec_receive_packet(this->codec_context, packet)) {
            case 0:
                /// use packet, copy/send it's data, or whatever
                std::cout << (packet->flags == 1 ? MAGENTA : YELLOW);
                
                std::cout << "PACKET " << packet->size
                          << " / " << packet->buf->size
                          << " pts=" << packet->pts
                          << " flags=" << packet->flags;
                std::cout << CLR;
                std::cout << std::endl;
                
                break;
            case AVERROR(EAGAIN):
                std::cerr << "Error receiving packet AVERROR(EAGAIN)" << std::endl;
                break;
            case AVERROR_EOF:
                std::cerr << "Error receiving packet AVERROR_EOF" << std::endl;
                break;
            case AVERROR(EINVAL):
                std::cerr << "Error receiving packet AVERROR(EINVAL)" << std::endl;
                break;
            default:
                std::cerr << "Error receiving packet" << std::endl;
                break;
        }

        ffmpeg_image_transport_msgs::msg::FFMPEGPacket outFrameMsg;
        std_msgs::msg::Header header;
        header.frame_id = "le_test";
        uint64_t timestamp_ns = metadata.timestamp;
        header.stamp = get_current_stamp(timestamp_ns);
        outFrameMsg.header = header;
        outFrameMsg.width = this->codec_context->width;
        outFrameMsg.height = this->codec_context->height;
        outFrameMsg.encoding = "h.264";
        outFrameMsg.pts = header.stamp.sec * 1000000000 + header.stamp.nanosec;
        outFrameMsg.flags = packet->flags;
        outFrameMsg.is_bigendian = false;
        // outFrameMsg.data = &packet->data; // uint8[] out
        // outFrameMsg.data = std::vector<uint8_t>(packet->size);
        outFrameMsg.data.assign(packet->data, packet->data + packet->size);

        std::cout << GREEN << " >> Sending " << outFrameMsg.data.size() << "B" << CLR << " sec: " << outFrameMsg.header.stamp.sec << " nsec:" << outFrameMsg.header.stamp.nanosec << std::endl;
    
        this->publisher->publish(outFrameMsg);

        av_packet_unref(packet);
        av_packet_free(&packet);   
    }

    request->reuse(Request::ReuseBuffers);
    this->camera->queueRequest(request);
}

CameraInterface::~CameraInterface() {
    this->camera->release();
    this->camera = NULL;
    this->node = NULL;
    avcodec_free_context(&this->codec_context);
}